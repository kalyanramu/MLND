import random
import math
from environment import Agent, Environment
from planner import RoutePlanner
from simulator import Simulator
from time import sleep
class LearningAgent(Agent):
    """ An agent that learns to drive in the Smartcab world.
        This is the object you will be modifying. """ 

    def __init__(self, env, learning=False, epsilon=1.0, alpha=0.5):
        super(LearningAgent, self).__init__(env)     # Set the agent in the evironment 
        self.planner = RoutePlanner(self.env, self)  # Create a route planner
        self.valid_actions = self.env.valid_actions  # The set of valid actions

        # Set parameters of the learning agent
        self.learning = learning # Whether the agent is expected to learn
        self.Q = dict()          # Create a Q-table which will be a dictionary of tuples
        self.epsilon = epsilon   # Random exploration factor
        self.alpha = alpha       # Learning factor

        ###########
        ## TO DO ##
        ###########
        # Set any additional class parameters as needed
        self.no_trials = 1
        random.seed(90)

    def reset(self, destination=None, testing=False):
        """ The reset function is called at the beginning of each trial.
            'testing' is set to True if testing trials are being used
            once training trials have completed. """

        # Select the destination as the new location to route to
        self.planner.route_to(destination)
        
        ########### 
        ## TO DO ##
        ###########
        # Update epsilon using a decay function of your choice
        # Update additional class parameters as needed
        # If 'testing' is True, set epsilon and alpha to 0
        
        self.no_trials += 1 #reset is called beginning of each trial
        self.a = 0.01 # 0.025 --> 1, decrease alpha and epsilon slowly
        #Update epsilon
        if testing is True: #self.epsilon < self.tolerance: # i.e. testing is true
            self.epsilon = 0
            self.alpha = 0
        else :
            #self.epsilon -=0.05 #Unimproved learning
            #self.epsilon -=0.005 #Improved learning
            
            #----Other experiments-----------#
            #self.epsilon = math.e**(-0.001*self.no_trials)
            #self.epsilon = 1-(0.025*self.no_trials)
            self.epsilon = float(1)/math.exp(float(self.a*self.no_trials)) #Improved Learning
            #self.alpha =float(1)/math.exp(float(self.a*self.no_trials)) #Improved Learning
            
        return None

    def build_state(self):
        """ The build_state function is called when the agent requests data from the 
            environment. The next waypoint, the intersection inputs, and the deadline
            are all features available to the agent. """
        
        # Collect data about the environment
        waypoint = self.planner.next_waypoint() # The next waypoint 
        inputs = self.env.sense(self)           # Visual input - intersection light and traffic
        deadline = self.env.get_deadline(self)  # Remaining deadline

        ########### 
        ## TO DO ##
        ###########
        # Set 'state' as a tuple of relevant data for the agent
        # When learning, check if the state is in the Q-table
        #   If it is not, create a dictionary in the Q-table for the current 'state'
        #   For each action, set the Q-value for the state-action pair to 0        

        # features refer to inputs
        #state = (waypoint,inputs['light'], inputs['left'], inputs['right'], inputs['oncoming'])
        state = (waypoint,inputs['light'], inputs['left'], inputs['oncoming'])
        #print("state:", state)
        stateID = self.getID(state)
        state_actions_init = {action: 0.0 for action in self.env.valid_actions}
        if stateID not in self.Q and self.learning:
            self.Q[stateID] = state_actions_init
        
        #sleep(1)
        return state


    def get_maxQ(self, state):
        """ The get_max_Q function is called when the agent is asked to find the
            maximum Q-value of all actions based on the 'state' the smartcab is in. """

        ########### 
        ## TO DO ##
        ###########
        # Calculate the maximum Q-value of all actions for a given state
        stateID = self.getID(state)
        state_Qvals = self.Q[stateID]       
        
        #Get Max Values of Q
        maxQ_key = max(state_Qvals, key = lambda key:state_Qvals[key])
        return state_Qvals[maxQ_key]
    
    #User defined function
    def getID(self, state):
        state_list = [str(s) for s in state ]
        stateID = ','.join(state_list) # state_list = list(state)
        return stateID
        
    def createQ(self, state):
        """ The createQ function is called when a state is generated by the agent. """

        ########### 
        ## TO DO ##
        ###########
        # When learning, check if the 'state' is not in the Q-table
        # If it is not, create a new dictionary for that state
        #   Then, for each action available, set the initial Q-value to 0.0
        stateID = self.getID(state)
        #print("state before action:", stateID)
        state_actions_init = {action: 0.0 for action in self.env.valid_actions}
        if stateID not in self.Q.keys():
            self.Q[stateID] = state_actions_init
        return


    def choose_action(self, state):
        """ The choose_action function is called when the agent is asked to choose
            which action to take, based on the 'state' the smartcab is in. """

        # Set the agent state and default action
        self.state = state
        self.next_waypoint = self.planner.next_waypoint()
        #action = None
       

        ########### 
        ## TO DO ##
        ###########
        # When not learning, choose a random action
        # When learning, choose a random action with 'epsilon' probability
        #   Otherwise, choose an action with the highest Q-value for the current state
        

        if self.learning == False: 
        #random action selection policy if the agent is not a learning one
            action = self.valid_actions[random.randint(0, len(self.valid_actions)-1)] 
        else:
            #http://outlace.com/Reinforcement-Learning-Part-1/
            #epsilon-greedy algorithm => we choose random/explore action e times and best action (1-e) times
            print("epsilon {}".format(self.epsilon))
            if random.random() > self.epsilon:
                #greedy arm selection: MaxQ selection
                maxQ_val = self.get_maxQ(state)
                
                #Adding code so that if all the values are zero, it will pick random action rather than the first items
                #in the action list, in the current code, it was always picking right turn, so need to change that
                #This step increased reliability but drove down safety
                stateID = self.getID(state)
                action_max_inds = [key for key in self.Q[stateID] if self.Q[stateID][key] == maxQ_val]
                #print(maxQ_val, action_max_inds)
                action = random.choice(action_max_inds)
                
                #debug
                #print("Debug, Q-values: ", self.Q[stateID])
                #print("Debug, Action taken from maxQ:", action)                
            else:
                #random exploration
                action = random.choice(self.valid_actions) #Take random action
 
        return action


    def learn(self, state, action, reward):
        """ The learn function is called after the agent completes an action and
            receives an award. This function does not consider future rewards 
            when conducting learning. """

        ########### 
        ## TO DO ##
        ###########
        # When learning, implement the value iteration update rule
        #   Use only the learning rate 'alpha' (do not use the discount factor 'gamma')
        gamma = 0
        #Formula: https://en.wikipedia.org/wiki/Q-learning
        #Q(s,a) = Q(s,a) + alpha * [R(s,a) + gamma * max(Q(s', a')) - Q(s, a)]
        stateID = self.getID(state)
        Q_val = self.Q[stateID][action]
        if self.learning:
            self.Q[stateID][action] = (1-self.alpha)*Q_val + self.alpha*reward;
        
        return


    def update(self):
        """ The update function is called when a time step is completed in the 
            environment for a given trial. This function will build the agent
            state, choose an action, receive a reward, and learn if enabled. """

        state = self.build_state()          # Get current state
        self.createQ(state)                 # Create 'state' in Q-table
        action = self.choose_action(state)  # Choose an action
        reward = self.env.act(self, action) # Receive a reward
        self.learn(state, action, reward)   # Q-learn
        return
        

def run():
    """ Driving function for running the simulation. 
        Press ESC to close the simulation, or [SPACE] to pause the simulation. """

    ##############
    # Create the environment
    # Flags:
    #   verbose     - set to True to display additional output from the simulation
    #   num_dummies - discrete number of dummy agents in the environment, default is 100
    #   grid_size   - discrete number of intersections (columns, rows), default is (8, 6)
    env = Environment()
    
    ##############
    # Create the driving agent
    # Flags:
    #   learning   - set to True to force the driving agent to use Q-learning
    #    * epsilon - continuous value for the exploration factor, default is 1
    #    * alpha   - continuous value for the learning rate, default is 0.5
    agent = env.create_agent(LearningAgent, learning = True, epsilon = 1, alpha = 0.55)
    
    ##############
    # Follow the driving agent
    # Flags:
    #   enforce_deadline - set to True to enforce a deadline metric
    env.set_primary_agent(agent, enforce_deadline = True)

    ##############
    # Create the simulation
    # Flags:
    #   update_delay - continuous time (in seconds) between actions, default is 2.0 seconds
    #   display      - set to False to disable the GUI if PyGame is enabled
    #   log_metrics  - set to True to log trial and simulation results to /logs
    #   optimized    - set to True to change the default log file name
    sim = Simulator(env, display = False, update_delay = 0.0001, log_metrics = True, optimized = True)
    
    ##############
    # Run the simulator
    # Flags:
    #   tolerance  - epsilon tolerance before beginning testing, default is 0.05 
    #   n_test     - discrete number of testing trials to perform, default is 0
    sim.run(n_test = 100, tolerance = 0.02) 

if __name__ == '__main__':
    run()
